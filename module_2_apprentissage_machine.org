#+LANGUAGE: fr
#+OPTIONS: title:nil toc:nil

* INF 1901 - Module 2 - Apprentissage machine
** Quels sont les objectifs de ce module?

** Qu'est-ce que l'apprentissage machine?

L'apprentissage machine (AM) est un ensemble de techniques
mathématiques qui permettent de résoudre des problèmes ardus en
informatique, souvent associés à l'intelligence artificielle (IA) :
classifier ou reconnaître des images (est-ce un chien ou un chat?),
prédire la valeur d'une maison, jouer aux échecs, converser en anglais
et résoudre des problèmes généraux, etc.

Ces problèmes sont considérés difficiles car il serait ardu d'écrire
un programme classique pour les résoudre. Un programme classique
encode essentiellement une série de règles et de procédures logiques
pour résoudre un problème, tandis qu'un modèle d'AM dérive plutôt sa
solution à partir d'exemples. Le fait qu'on parle d'intelligence de
manière plus explicite dans le cas d'un modèle d'AM (par rapport à un
programme classique) est un peu arbitraire, et matière à débat. Il
reste que fondamentalement, l'AM est associée à des courants
philosophiques, comme le connexionnisme par exemple, qui sont
généralement associés à l'étude de l'intelligence humaine ou animale.

#+CAPTION: Problème classique versus problème d'apprentissage
#+ATTR_LATEX: :width 0.8\textwidth
[[file:./images/abeille.png]]

En général, l'apprentissage machine utilise des données (qu'on appelle
parfois des exemples) pour "entrainer" un modèle à l'aide d'un
algorithme d'apprentissage. Une fois l'entrainement accompli, on peut
utiliser l'algorithme dans un contexte où c'est utile. Le modèle est
dynamique et changeant seulement dans la phase d'entrainement, à la
phase d'utilisation, il est un objet statique.

La notion probablement la plus profonde et philosophique de l'AM, et
celle qui fait en sorte qu'on rattache ce domaine à l'IA, est qu'un
algorithme d'apprentissage devrait être en mesure de généraliser : si
j'ai entrainé un modèle à distinguer entre un chien et un chat avec
1000 images d'entrainement, je ne suis pas intéressé par la
performance du modèle sur l'une des images particulières qui ont servi
à l'entrainement. Par construction et quasiment par définition, cette
classification particulière devrait être correcte. Je suis plutôt
intéressé par la classification de la 1001ième image, qui n'a pas
servi à l'entrainement du modèle, et qui est donc entièrement
nouvelle. Si le modèle a été entrainé avec succès, il devrait pouvoir
généraliser à n'importe quelle image (par contre, la question se pose
à savoir ce qui devrait arriver si je lui présente une image d'une
vache!). Une bonne capacité de généralisation est le but fondamental
de l'AM et de l'IA en général, et est reliée à ce qu'on entend par
intelligence, scientifiquement parlant.

** L'apprentissage machine dans un vrai scénario industriel

*** Situation

Imaginez une compagnie où il y a une chaine de montage où on assemble
des téléviseurs.

*** Le problème 

Supposons que dans un endroit particulièrement délicat de la chaine de
montage, un problème survienne parfois, que l'on aimerait détecter le
plus rapidement possible.

*** Une solution possible

On pourrait imaginer placer une caméra vidéo dont le but serait de
visionner le flot des appareils en cours d'assemblage, pour tenter de
détecter les problèmes. Pour ce faire, la caméra pourrait transmettre,
à intervalles réguliers, les pixels de ce qu'elle capte, en tant que
donnée à un modèle d'apprentissage machine qui roulerait (en tant que
programme) sur un serveur, pas très éloigné. Ce modèle convertirait
les pixels de la caméra en tant que données numériques (les entrées,
"inputs"), et effectuerait un calcul complexe sur ces valeurs, en vue
nde produire une valeur de sortie simple ("outputs") : "oui il y a un
problème avec cette image", ou "non il n'y a pas de problème avec
cette image". Sur la base de cette valeur de sortir, on pourrait agir
et envoyer un technicien, en cas de besoin.

*** Quelle sera la nature de ce modèle?

Ce modèle sera essentiellement un modèle au sens statistique classique
du terme : une série de paramètres déterminant une fonction
particulière. Par analogie avec une fonction classique qu'on apprend
au secondaire :

#+BEGIN_EXPORT latex
\[
f(x) = mx + b
\]
#+END_EXPORT

$m$ et $b$ (les valeurs particulières qu'on leur donne) sont les paramètres qui déterminent cette fonction particulière. Dans un modèle d'apprentissage machine, il y a beaucoup plus de paramètres, mais c'est essentiellement la même idée. Les paramètres d'un modèle sont donc essentiellement une série de nombres, rien de plus. Notre but est maintenant de trouver une manière de calculer la valeur exacte de ces paramètres pour notre modèle. Pour commencer, notre modèle a des valeurs aléatoires pour ses paramètres.

*** Qu'est-ce qu'un ensemble de données d'entrainement?

Nous avons tout d'abord besoin d'un ensemble de données
d'entrainement, qui est constitué d'une série d'images, accompagnées
chacune d'une étiquette "oui c'est un problème", ou "non ce n'est pas
un problème". Évidemment dans la réalité les problèmes sont plus
rares, mais il faudrait idéalement que cet ensemble d'entrainement (de
disons 1000 images) soit constitué à 50% de cas problématiques, et 50%
de cas non-problématiques.

*** Qu'est-ce que la fonction d'erreur?

La fonction d'erreur détermine l'erreur moyenne qu'une version donnée du modèle (avec des valeurs précises pour les paramètres) entraine. S'il y a 1000 images, dont 500 images "oui", et 500 images "non, et que le modèle répond "non" pour les 1000, alors il a fait 500 erreurs.

*** Qu'est-ce que l'entrainement (ou l'optimisation de la fonction d'erreur)?

La partie cruciale est ici : on aimerait une procédure qui va changer la valeur des paramètres (qui au départ sont des valeurs aléatoires) de manière à réduire l'erreur, idéalement à zéro. Parfois il est possible
de trouver les bonnes valeurs pour les paramètres "d'un coup", mais plus
souvent, il est plus pratique de le faire progressivement. La valeur de
la fonction d'erreur va donc diminuer graduellement, la fonction va donc
être "optimisée".

*** Qu'est-ce que l'inférence (ou l'utilisation du modèle dans la réalité)?

Une fois les bonnes valeurs pour les paramètres trouvées, la tâche est
accomplie, le modèle est prêt à être utilisé dans la réalité. On
conserve donc précieusement les valeurs de ces paramètres, et on les
place dans une version "officielle" du modèle, qui devra traiter des
données provenant de la chaine de montage. Ces données sont
"nouvelles", dans le sens qu'elles n'ont pas servies à l'entrainement
du modèle (elles ne feront nécessairement pas partie de l'ensemble des
1000 images d'entrainement). Mais notre espoir est que le modèle aura
appris à "généraliser", à partir des exemples qu'il aura vus pendant
son entrainement. Si jamais le modèle ou la couleur des téléviseurs
changent, il est possible que notre modèle se comporte moins bien, et
fasse donc plus d'erreurs. Il sera donc peut-être nécessaire de
procéder à son réentrainement.

** En quoi l'AM diffère de la programmation traditionnelle?

Bien que l'apprentissage machine requiert de la programmation, il
s'agit d'un paradigme entièrement différent de celui de la
programmation.

Un programme traditionnel spécifie une série d'instructions que
l'ordinateur exécute pour résoudre un problème. Normalement, ce
programme fait son travail en relation avec des données fournies par
l'utilisateur. Le programme dans ce cas est une série d'instructions
symboliques dans un langage de programmation.

Un modèle d'AM (déjà entrainé) va prendre en entrée des données
fournies par l'utilisateur, et va fournir une réponse appropriée après
avoir effectué une série d'opérations mathématiques. Si on veut
absolument parler de "programme" dans ce cas, on peut parler des
opérations mathématiques (pas nécessairement symboliques) qui sont
effectuées sur les données, pour les transformer en réponse. Il est
important de comprendre que même si un modèle d'AM est avant tout un
objet mathématique (un modèle avec ses paramètres), son implémentation
concrète se fait quand même toujours avec un langage de programmation.

** En quoi l'AM diffère de l'IA?

L'intelligence artificielle est le domaine plus vaste, qui englobe
l'apprentissage machine. Les deux ont des méthodes profondément
différentes, et l'histoire de leur développement est entièrement
différente. Dans un certain sens, l'AM est une forme plus spécialisée
et un peu plus récente d'IA, plus mathématique, moins symbolique, et
clairement celle qui domine la période actuelle.

** En quoi l'AM diffère des statistiques?

L'apprentissage machine, conceptuellement, est pratiquement identique
aux statistiques. Dans les deux cas on parle de modèles,
d'entrainement (ou recherche des paramètres), d'inférence, etc.
Toutefois l'AM est plus axée sur les problèmes dont la modélisation se
fait en très haute dimension, comme l'analyse d'images ou le
traitement du langage. De plus, l'accent en AM est davantage mis sur
les aspects computationnels, par opposition aux mathématiques (bien
que le AM demeure très mathématique en substance).

** Comment représenter les données

Un problème crucial qui se pose en AM est comment adéquatement
représenter les données, pour qu'elles soient traitables et
compréhensibles à la fois par l'ordinateur ainsi que le modèle (ou
algorithme) d'apprentissage qu'on veut utiliser. Il existe de
nombreuses manières de faire cela, mais un thème récurrent est
l'utilisation d'espaces vectoriels pour représenter les données, ce
qui est très étroitement relié au fait que la plupart des techniques
d'AM touche de près ou de loin l'algèbre linéaire. Une image, par
exemple, sera un point dans un espace vectoriel à très haute dimension
(autant de dimensions qu'il y a de pixels!), et un mot pourrait être
un point dans un espace vectoriel extrêmement épars (sparse) pour
représenter la présence ou l'absence d'un mot. Il est également
possible de représenter le sens des mots à l'aide d'un espace
vectoriel, dont les grands modèles de langage (GML) font usage.

On parle souvent de "features" en AM, qui sont les caractéristiques,
souvent numériques, mais pas toujours, des instances, ou des objets
que l'on tente de traiter. Classiquement, on fait de l'ingénierie de
features sur les données, pour tenter de les transformer de manière à
améliorer les performances d'un algorithme. Le AM très moderne qui
utilise les réseaux de neurones profonds tend à faire en sorte qu'on a
moins besoin de ce genre de techniques, car les transformations sont
faites automatiquement, par le réseau de neurones lui-même.

** Les différents paradigmes de l'AM

Il existe plusieurs manières de catégoriser les algorithmes
d'apprentissage machine, selon la nature et la structure des problèmes
qu'ils tentent de résoudre. La catégorisation suivante est très
classique.

*** Apprentissage supervisé (classification, regression)

L'apprentissage supervisé fonctionne à partir de données pour
lesquelles la "bonne réponse" (i.e. celle qu'on aimerait que
l'algorithme donne systématiquement) est fournie, en tant que donnée
d'entrainement.

**** Régression

Une régression est une famille d'algorithmes d'apprentissage supervisé
(ou plus classiquement, de modélisation statistique) dont le but est
de découvrir une fonction numérique continue, au sens classique
mathématique (dans sa forme la plus simple, une fonction associe une
valeur numérique du domaine X vers l'image Y).

- Régression linéaire (ex. nombre de pièces, année de construction -> prix d'une maison)

**** Classification  

Une autre famille d'algorithmes d'apprentissage supervisé tente plutôt de
découvrir une fonction de classification, qui associe une série de features
à une catégorie particulière (dont le nombre est fini et connu d'avance).

- Régression logistique (ex. nombre d'heures étudiées, nombre de cours -> étudiant a gradué ou non)
- k-NN
- Arbres de décision
- Naive Bayes

*** Apprentissage non-supervisé

L'apprentissage non-supervisé fonctionne à partir de données pour
lesquelles la "bonne réponse" n'est pas fournie. Les algorithmes de
cette famille doivent donc découvrir la structure inhérente aux
données, de manière autonome, tout en étant guidé possible par des
hypothèses et ses "biais inductifs".

**** Partitionnement (clustering)

Avec un algorithme de partitionnement, on peut découvrir des
"agrégats", ou des groupes naturels dans les données.

- k-Means
- DBScan
- Hierarchical clustering

**** Réduction de la dimensionnalité

En tentant de réduire la dimensionnalité des données, on peut
découvrir sa structure inhérente, ce qui est souvent utile en
visualisation (par exemple, une donnée exprimée en très haute
dimension peut être plus facile à comprendre en 2d ou 3d).

- PCA

*** Apprentissage par renforcement (RL)

L'apprentissage par renforcement (APR) est un paradigme différent des
deux précédents. Si les apprentissages supervisé et non-supervisé
pourraient être qualifiés de perceptifs (quelle est la nature de ce
que je perçois?), l'apprentissage par renforcement pourrait être
compris en tant que modélisation behaviorale (quelle action devrait
être posée dans ce contexte particulier). L'APR est souvent utilisé
dans les jeux et la robotique.

** Réseaux de neurones

Les réseaux de neurones sont un algorithme d'apprentissage
classiquement supervisé (mais cela va au-delà) extrêmement puissant et
versatile, qui est l'élément clé à la base des révolutions de
l'apprentissage profond et de l'IA génératif des temps récents. L'idée
est de faire passer les données représentées à travers une série de
couches de neurones, connectées par des matrices de poids (nombres
réels), de manière à les transformer de manière extrêmement complexe
et non-linéaire, afin de pouvoir découvrir des associations
extrêmement sophistiquées et subtiles entre les données d'entrée (par
exemple le prompt de ChatGPT) et les données de sortie (sa réponse).
Le nombre de couches internes fait en sorte que ces réseaux sont
qualifiés de "profonds", ce qui mène à l'apprentissage profond (deep
learning).

** Les applications de l'AM

- Modélisation
- Tests médicaux
- Jeux
- Chatbot
- Etc.

** Concepts

*** Données

*** Représentation

*** Paramètres

*** Fonction objective (d'erreur)

*** Entrainement

*** Généralisation

*** Algorithme

*** Implémentation

*** Ingénierie des caractéristiques (feature engineering)

Quelles sont les composantes d'un NN?
- Noeuds
- Poids
- Fonction d'erreur
- Optimiseur (qui opt la fonction d'erreur)
  
